\documentclass[11pt]{article}
\usepackage{bookmark}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{color}
\usepackage{comment}
\usepackage{float}
\usepackage{graphicx}
%\usepackage[hidelinks]{hyperref}
\usepackage{makecell}
\usepackage[caption=false,font=footnotesize,subrefformat=parens,labelformat=parens]{subfig}
\usepackage{wrapfig}
\usepackage{url}
\usepackage[table]{xcolor}
%
\setlength{\parindent}{0.25in}
\setlength{\parskip}{.05in}
\pagestyle{plain}
%Title, date an author of the document
\title{Progress Report}
\author{Bardia Mojra}


\begin{document}
\maketitle
\thispagestyle{empty}

\bigskip
\bigskip
\begin{center}
	Robotic Vision Lab
\end{center}

\begin{center}
	The University of Texas at Arlington
\end{center}

\newpage

\section{To Do}
\begin{itemize}
	\item Implement basic estimation and generate a baseline for incremental
	      advancement: Currently, I can train a basic model using  the randomly
	      generated data. I am currently working on resolving a visualization
	      issue.
	\item Implement DOPE with added dropout before each layer to estimate
	      variational Bayesian inference.
	\item Read Data Association and Localization of Classified Objects in Visual
	      SLAM: in progress.
	\item Work on Pose Estimation Uncertainty paper.
	\item Implement PoseCNN, DOPE, and BayesOD.
	\item Look into domain randomization and adaptation techniques.
	\item Search for recent pose estimation survey papers: Found this paper,
	      \cite{du2020vision}, on pose estimation.
	\item As I read more papers, go back and update Pose Estimation Survey paper.
	\item Finish Tensorflow tutorials, \cite{CVTF2}: 4/9 chapters done.
	\item Learn to implement transfer learning.
	\item (On pause) Finish Docker tutorials, \cite{schenker2020learn}: 4/18
	      chapters done.
\end{itemize}

\section{Literature Reviews}
\subsection{Dropout as a Bayesian Approximation: Representing Model Uncertainty
	Deep Learning}
\par In this paper \cite{gal2016dropout}, the authors introduce a mathematical
framework that casts training deep neural nets with dropout layers as
approximate Bayesian inference in deep Gaussian processes. They show that dropout
layers in deep networks can minimize Kullback-Leibler divergence between an
approximate distribution and the posterior of a deep Gaussian process, \cite{damianou2013deep}.
This is quite effective since dropout layers preserve Gaussian processes'
variational distributions.

\par To make posterior weight distribution \(p(\omega|D)\) intractable, columns
are set to zero at random and we end up with \(q(\omega)\), an approximate
posterior weight distribution. Moreover,these layers are highly multi-modal and
induces strong joint correlations over the rows of the network weight matrices
\(W_i\).

\par The objectice KL divergence minimization is computed by approximating each
term by Monte Carlo integration with a single sample,\(\widehat{\omega}_n
\sim q(\omega)\).
This allows for unbiased estimation of each posterior modality with
\(-log~p(y_n | x_n,\widehat{\omega_n} ) \) combined with the second summation
term they obtained the following:
$$
	\mathcal{L}_{GP-MC} \propto\frac{1}{N}\sum_{n = 1}^{N}
	\frac{-log~p(y_n | x_n,\widehat{\omega}_n )}{ \tau }
	+ \sum_{l = 1}^{L} (\frac{p_i l^2}{2 \tau N} \|\textbf{M}_i\|_{2}^{2}
	+  \frac{l^2}{2 \tau N} \|\textbf{m}_i \|_{2}^{2})
$$

Where
$$
	E(y_n, \widehat{y}(x_n, \widehat{\omega}_{n})) = -log~p(y_n | x_n,\widehat{\omega}_n )/{ \tau}
$$

and approximate predictive distribution function represented by:
$$
	q(y^* | x^*) = \int p(y^* | x^*, \omega) q(\omega)d\omega
$$

Which means this Monte Carlo estimation, \textit{MC Dropout}, makes generalized
predictions by averaging the weights of the network. This would be similar to T
stochastic forward pass through the network and averaging the outputs but rather
computationally more efficient.

For regression tasks, predictive log-likelihood estimate is given by:
$$
	log~ p(y^* | x^*, X,Y)\approx  logsumexp(-\frac{1}{2} \tau \|y-\widehat{y}_t\|^2)
	- log~T - \frac{1}{2} log~ 2 \pi - \frac{1}{2} log~\tau^{-1}
$$

In their experiments, they used a LeNet-5 with dropout layers added. For
regression, MC Dropout with .1 or.2  probabilities (resulted in identical
uncertainties) were applied before every weight layer and with TanH activation.
For classification, they applied a dropout layer before the last fully connected
inner-product layer. They use probability of .5 for dropout layers in
classification tasks.


\section{Progress}
Following items are listed in order of priority:
\begin{itemize}
	\item Object Pose Estimation with Uncertainty: I started a Tensorflow
	      \cite{tensorflow2015-whitepaper} project based on a simple implementation
	      \cite{graphics85:online}. The code is written using Tensorflow Graphics
	      module which is currently on supported by Google Colab, I am in the
	      process of porting the code so it will use TensorBoard 3D instead.
	      Next, I will expand the 3-layer nextwork to five layers, train, test
	      and compare the results. Later, I will add dropout layers for uncertainty
	      analysis in regression.

	      I am currently restructuring the code into modules for model build, train,
	      and testing. Other modules such as feature extraction can also be added
	      to further enhance the performance.


	\item Implement PoseCNN, DOPE, and BayesOD.
	\item Look into domain randomization and adaptation techniques.
	\item Search for recent pose estimation survey papers: Found this paper,
	      \cite{du2020vision}, on pose estimation.

\end{itemize}

%\newpage

\section{Plans}
Following items are listed in order of priority:

\begin{itemize}

	\item Implement multiple object pose estimation with uncertainty estimation.
	\item Keep working on Bayesian Pose Estimation paper.
	\item ARIAC: For now, I will focus on implementing pose estimation and
	      BayesOD implementations.

	\item Continue on UE4 tutorials.

	\item Pose Estimation Survey Paper Feedback: On hold, I am working on
	      Bayesian Pose Estimation.
	\item Project Alpe with Nolan: On pause for right now.
	\item UR5e: Finish ROS Industrial tutorials.
\end{itemize}

\section{2021 Goals and Target Journals/Conferences}
\begin{itemize}
	\item Submit a paper on pose estimation with uncertainty to ICIRS.
	\item Get comfortable with TensorFlow and related Python modules.
	\item Keep writing.
\end{itemize}


%Sets the bibliography style to UNSRT and import the
\newpage
\bibliography{bib_BardiaMojra_dd-mm2020}
\bibliographystyle{ieeetr}

\end{document}
